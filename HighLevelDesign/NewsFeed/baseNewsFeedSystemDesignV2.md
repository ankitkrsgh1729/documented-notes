# News Feed System Design - Complete Guide

> **Note:** For detailed explanation of pagination, cursors, versioning, and checksums, see [Feed Pagination & Consistency Guide](feedPaginationViaCursorAndChecksum.md)

## System Overview & Requirements

### Functional Requirements
- Create and view posts (text, media)
- Follow/unfollow users
- View personalized chronological news feed
- Infinite scroll pagination
- Basic interactions (likes, comments)

### Non-Functional Requirements
- **Scale:** 2B users, 300M DAU, 150M posts/day
- **Performance:** Feed load <500ms, Post creation <200ms
- **QPS:** 35K reads/sec, 2K writes/sec
- **Storage:** 100TB annually
- **Consistency:** Eventual consistency (1-minute delay acceptable)

### Capacity Estimation
```
Daily Posts: 300M users Ã— 0.5 posts = 150M posts/day = ~1,736 posts/sec
Feed Requests: 300M users Ã— 10 requests = 3B requests/day = ~34,722 requests/sec
Storage: 150M Ã— 2KB = 300GB/day â‰ˆ 100TB/year
Cache: Multi-tier approach to handle 300M users efficiently
```

---

## High-Level Architecture

```
[User] â†’ [Load Balancer] â†’ [Web Servers] â†’ [Services] â†’ [Cache] â†’ [Database]
                                â†“
                        [Message Queues]
                                â†“
                        [Fanout Workers]
```

### Core Components

**Web Services Layer:**
- Post Service: Handle post creation and retrieval
- Feed Service: Manage user feed generation and pagination
- Graph Service: Handle follow/unfollow operations
- Authentication Service: User management and security

**Backend Services:**
- **Fanout Service:** Orchestrates feed distribution after new posts
- **Message Queues:** Handle asynchronous processing (post_created_events, feed_update_jobs)
- **Fanout Workers:** Process feed updates asynchronously

**Storage Layer:**
- **Multi-tier Cache:** Redis (Tier 1), Memcached (Tier 2)
- **Databases:** Post DB, User DB (DynamoDB), Graph DB (Neo4j/Neptune)


#### Memcached vs Redis vs Database - Quick Comparison

| Aspect | Memcached | Redis | Database |
|--------|-----------|-------|----------|
| **Storage Location** | RAM only | RAM (+ optional disk backup) | Disk (+ RAM cache) |
| **Data Persistence** | âŒ No (lost on restart) | âœ… Yes (can save to disk) | âœ… Yes (permanent) |
| **Data Structures** | String â†’ String only | String, List, Set, Hash, Sorted Set, etc. | Tables, Rows, Columns |
| **Speed** | Very Fast (~1ms) | Very Fast (~1ms) | Slower (~10-50ms) |
| **Threading** | Multi-threaded | Single-threaded | Multi-threaded |
| **Max Value Size** | 1 MB | 512 MB | Unlimited |
| **TTL (Expiration)** | âœ… Yes | âœ… Yes | Manual cleanup |
| **Transactions** | âŒ No | âœ… Yes | âœ… Yes (ACID) |
| **Replication** | âŒ No (built-in) | âœ… Yes | âœ… Yes |
| **Use Case** | Simple cache, cheap | Cache + real-time features | Permanent storage, complex queries |
| **Cost (AWS)** | $0.017/GB/hour | $0.034/GB/hour | Varies (slower, cheaper per GB) |
| **When Data Lost** | Gone forever | Can recover from disk | Never lost |

---
Even though Redis stores data in RAM for speed, it can periodically save snapshots to disk. Hence recovery from disk is possible.
#### Simple Mental Model

```
Memcached:  Whiteboard (fast, temporary, simple)
Redis:      Smart whiteboard (fast, temporary, many features)
Database:   Filing cabinet (slower, permanent, organized)
```

---


## Data Flow Patterns

### Post Creation Flow (Write Path)

```
1. User creates post â†’ Post Service
2. Post Service saves to Post DB + Post Cache
3. Publish "post_created_event" to message queue
4. Fanout Service consumes event â†’ Queries Graph DB for followers
5. Fanout Service publishes "feed_update_jobs" to queue
6. Fanout Workers consume jobs â†’ Update News Feed Cache for each follower
```

**Key Points:**
- Asynchronous processing prevents blocking user's post creation
- Message queue decouples post creation from fanout
- Fanout workers can be scaled independently based on load

### Feed Generation Flow (Read Path)

```
1. User requests feed â†’ Feed Service
2. Check News Feed Cache (multi-tier lookup)
3. If cache hit â†’ Return cached feed with pagination
4. If cache miss â†’ Generate from Post DB + Graph DB
5. Store in appropriate cache tier â†’ Return to user
```

**Key Points:**
- Multi-tier cache lookup (Redis â†’ Memcached â†’ Database)
- Cache miss triggers feed generation from database
- Generated feed cached for future requests

---

## Multi-Tier Caching Strategy

### Cache Distribution

**Redis (Tier 1) - Hot Users (30M users)**
- Active users (last hour)
- Stores rich metadata: version, checksum, session info
- Supports real-time features and WebSocket sessions
- Size: 30M Ã— 2.5KB = 75GB

**Memcached (Tier 2) - Warm Users (100M users)**  
- Recent users (last 24 hours)
- Stores only serialized posts (no metadata)
- Simple key-value storage
- Size: 100M Ã— 2KB = 200GB

**Database (Tier 3) - Cold Users (170M users)**
- Inactive users
- Generate feeds on-demand from Post DB + Graph DB
- No pre-computed storage

### Tier Decision Logic

```
User Activity Check:
â”œâ”€â”€ Last seen < 1 hour â†’ Redis (Hot)
â”œâ”€â”€ Last seen < 24 hours â†’ Memcached (Warm)  
â””â”€â”€ Last seen > 24 hours â†’ Database (Cold)

Dynamic Migration:
- Warm user requests real-time features â†’ Migrate to Redis
- Hot user becomes inactive â†’ Demote to Memcached
```

### Why Multi-Tier Instead of Single Redis?

**Cost Optimization:**
- Redis: $0.05/GB/hour â†’ 300M users would cost $30K/hour
- Memcached: $0.01/GB/hour â†’ Significant savings for warm data
- Most users don't need real-time features

**Feature Optimization:**
- Redis: Complex data structures, pub/sub, WebSocket sessions
- Memcached: Simple key-value, perfect for serialized post lists
- Right tool for the right job

---

## Feed Consistency & Pagination

> **ðŸ“– Detailed Guide:** See [Feed Pagination & Consistency Guide](feedPaginationViaCursorAndChecksum.md) for complete explanation with diagrams

### Quick Summary

**Cursor-Based Pagination:**
- Format: `"timestamp_postID"` (e.g., `"2025-01-15T10:20:00Z_post_C"`)
- Stable reference that doesn't shift when new posts arrive
- Handles duplicates automatically

**Feed Versioning:**
- Each feed state has unique version ID
- Detect when feed changed during user session
- Used mainly for UX (showing "new posts available" banner)
- Also handles edge cases like post deletions

**Checksums:**
- XOR-based hash for O(1) integrity validation
- Detects cache corruption during updates
- Recovers by regenerating from database

### Feed Cache Structure
```json
{
  "version": "v1642678800",
  "checksum": "a1b2c3d4",
  "posts": [post_ids...],  // Max 200 posts
  "metadata": {
    "last_updated": "2025-01-15T10:30:00Z",
    "cursor_positions": {...}
  }
}
```

**TTL Strategy:**
- Redis (Hot): 1 hour TTL
- Memcached (Warm): 24 hours TTL
- Database (Cold): No caching

---

## Hybrid Push-Pull Strategy

### The Celebrity Problem

**Challenge:** User with 50M followers â†’ 50M cache updates per post (write amplification)

**Example:**
```
Celebrity posts once:
â”œâ”€â”€ Traditional Push: Update 50M feeds = 50M cache writes
â”œâ”€â”€ Queue depth: Massive backlog
â”œâ”€â”€ Time to complete: Hours
â””â”€â”€ Cost: Extremely high
```

### Fanout Decision Logic

```
Follower Count Strategy:
â”œâ”€â”€ < 100K followers â†’ PUSH (pre-compute all feeds)
â”‚   â””â”€â”€ Fast reads, manageable write amplification
â”‚
â”œâ”€â”€ 100K - 1M followers â†’ PUSH_ACTIVE (only active followers)
â”‚   â””â”€â”€ Skip inactive users, reduce fanout by ~70%
â”‚
â””â”€â”€ > 1M followers â†’ PULL (compute at read time)
    â””â”€â”€ No fanout, fetch at read time
```

### Hybrid Feed Generation (Read Time)

**For Regular User:**
```
1. Get precomputed feed from Redis/Memcached (push model)
2. Check if user follows any celebrities (Graph DB lookup)
3. If yes: Fetch recent celebrity posts separately (pull model)
4. Merge both feeds chronologically
5. Cache merged result
6. Return to user

Example Timeline:
[Celebrity Post X] â† Fetched via pull
[Friend Post A]    â† From precomputed feed
[Celebrity Post Y] â† Fetched via pull
[Friend Post B]    â† From precomputed feed
[Friend Post C]    â† From precomputed feed
```

**Benefits:**
- **Scalability:** No write amplification for celebrities
- **Performance:** Most users still get fast precomputed feeds
- **Flexibility:** Thresholds adjustable based on system load
- **Cost:** Significant reduction in cache updates

**Trade-off:**
- Slight read latency for celebrity followers (50-100ms extra)
- Acceptable since most feeds are still precomputed

---

## Real-Time Updates

### WebSocket Integration (Redis Tier Only)

**Why Only Redis Tier:**
- Real-time requires persistent WebSocket connections
- Need session management and connection metadata
- Memcached can't handle complex session state
- Database tier users too inactive to justify real-time

**Real-Time Flow:**
```
1. User A (celebrity) creates post
2. Fanout service identifies online followers in Redis tier
3. Send WebSocket notification: "New posts available"
4. Client shows banner: "5 new posts â†» Tap to refresh"
5. User chooses to refresh (pull-to-refresh)
6. Feed service merges new posts with existing feed
```

**Why Not Auto-Insert?**
- Disrupts user's reading experience
- Can cause UI jumpiness
- User loses their position
- Better UX to let user control refresh timing

### Graceful Degradation

**If WebSocket connection fails:**
```
Redis Tier Users:
â”œâ”€â”€ Fall back to HTTP polling (every 30 seconds)
â”œâ”€â”€ Still get updates, just not real-time
â””â”€â”€ System remains functional

Memcached Tier:
â”œâ”€â”€ Continue with batch fanout updates
â””â”€â”€ Updates appear on next feed refresh

Database Tier:
â”œâ”€â”€ No change (already on-demand generation)
â””â”€â”€ See latest posts when they refresh
```

---

## Database Design

### Posts Table (DynamoDB)

**Schema:**
```
Partition Key: post_id
Sort Key: created_at (for range queries)

Global Secondary Indexes:
1. author_id + created_at (fetch user's posts)
2. is_deleted + created_at (filter deleted posts)

Attributes:
- post_id (UUID)
- author_id (User ID)
- content (text, max 280 chars)
- media_urls (list of S3 URLs)
- created_at (timestamp)
- updated_at (timestamp)
- post_type (text/image/video)
- likes_count (number)
- comments_count (number)
- is_deleted (boolean)
```

**Query Patterns:**
```
1. Get post by ID: 
   Query by post_id (O(1) lookup)

2. Get user's posts:
   Query GSI: author_id + created_at DESC
   
3. Batch get posts for feed:
   BatchGetItem with list of post_ids
```

### Social Graph (Neo4j/Neptune)

**Graph Structure:**
```
Nodes: User
Relationships: FOLLOWS

Example:
(User A)-[:FOLLOWS]->(User B)
(User A)-[:FOLLOWS]->(User C)
(User D)-[:FOLLOWS]->(User A)
```

**Key Queries:**
```cypher
// Get user's followers (for fanout)
MATCH (u:User {id:'user123'})<-[:FOLLOWS]-(follower)
RETURN follower.id, follower.last_active
LIMIT 10000

// Get user's followings (for feed generation)
MATCH (u:User {id:'user123'})-[:FOLLOWS]->(following)
RETURN following.id

// Get active followers only (for PUSH_ACTIVE strategy)
MATCH (u:User {id:'celebrity'})<-[:FOLLOWS]-(f)  
WHERE f.last_active > timestamp() - 86400
RETURN f.id
```

**Why Graph Database?**
- Follow relationships are highly connected
- Need fast traversal for fanout operations
- Efficient for "followers of followers" queries
- Better than SQL joins for many-to-many relationships

### User Table (DynamoDB)

**Schema:**
```
Partition Key: user_id

Attributes:
- user_id (UUID)
- username (string)
- email (string)
- last_active (timestamp)
- follower_count (number)
- following_count (number)
- tier (hot/warm/cold)
- created_at (timestamp)
```

---

## Performance Optimizations

### Feed Generation Optimizations

**1. Parallel Processing**
```
Traditional: Sequential fetch
User's 100 followings â†’ 100 sequential DB calls â†’ 1000ms

Optimized: Parallel fetch
User's 100 followings â†’ 10 parallel batches â†’ 100ms
```

**2. Smart Batching**
```
Group similar requests:
- Batch get posts: BatchGetItem (get 100 posts in 1 call)
- Batch graph queries: Get all followings in single query
- Reduces network round trips
```

**3. Connection Pooling**
```
Without pooling: Create new connection per request
- Connection overhead: 50-100ms
- Max connections: Limited

With pooling: Reuse existing connections
- Connection overhead: 0ms
- Max connections: Configurable pool size
```

**4. Incremental Checksums**
```
Traditional: O(n) recalculation
- Recalculate hash for all 200 posts
- CPU intensive

XOR-based: O(1) updates
- Add post: checksum XOR hash(new_post)
- Remove post: checksum XOR hash(removed_post)
```

### Cache Optimizations

**1. Cache Warming**
```
During low traffic hours (2-4 AM):
- Identify trending users
- Pre-generate their followers' feeds
- Improves cache hit rate during peak hours
```

**2. Compression**
```
Without compression: 2KB per feed Ã— 130M users = 260GB
With compression: 1KB per feed Ã— 130M users = 130GB
- Use gzip or snappy
- 50% memory savings
```

**3. Lazy Loading**
```
Initial load: Only first 50 posts
User scrolls: Fetch next 50 posts
- Reduces memory usage
- Faster initial load time
```

**4. Dynamic Migration**
```
Monitor user activity:
- Inactive hot user (no activity for 1 hour) â†’ Demote to warm
- Active warm user (requests real-time) â†’ Promote to hot
- Optimizes cache utilization
```

---

## Scaling Challenges & Solutions

### 1. Write Amplification

**Problem:**
```
Celebrity with 50M followers posts once:
â”œâ”€â”€ Traditional push: 50M feed updates
â”œâ”€â”€ Queue depth: Hours of backlog
â”œâ”€â”€ Other users' posts delayed
â””â”€â”€ System overwhelmed
```

**Solution:**
```
Hybrid Strategy:
â”œâ”€â”€ Regular users (< 100K): Push model
â”œâ”€â”€ Popular users (100K-1M): Push to active followers only
â”œâ”€â”€ Celebrities (> 1M): Pull model at read time
â””â”€â”€ Result: Balanced write load
```

### 2. Hot Partitions

**Problem:**
```
Celebrity's posts â†’ Single database partition
â”œâ”€â”€ All followers query same partition
â”œâ”€â”€ Partition overloaded
â”œâ”€â”€ Slow queries for everyone
â””â”€â”€ Database becomes bottleneck
```

**Solution:**
```
Strategies:
â”œâ”€â”€ Separate celebrity posts to dedicated shards
â”œâ”€â”€ Use pull model (distributes reads across time)
â”œâ”€â”€ Cache celebrity posts aggressively
â”œâ”€â”€ Connection pooling to prevent connection exhaustion
â””â”€â”€ Read replicas for celebrity data
```

### 3. Memory Requirements

**Problem:**
```
300M users Ã— 2KB per feed = 600GB cache needed
- Single Redis cluster: Very expensive
- Memory pressure â†’ Evictions â†’ Cache misses
```

**Solution:**
```
Multi-tier approach:
â”œâ”€â”€ Hot users (30M): Redis 75GB
â”œâ”€â”€ Warm users (100M): Memcached 200GB
â”œâ”€â”€ Cold users (170M): No cache, on-demand
â””â”€â”€ Total: 275GB instead of 600GB (54% savings)
```

### 4. Feed Consistency

**Problem:**
```
User scrolling while new posts arrive:
â”œâ”€â”€ Page 1: [A, B, C]
â”œâ”€â”€ New post X inserted at top
â”œâ”€â”€ Page 2 request: Might see duplicate C
â””â”€â”€ Poor user experience
```

**Solution:**
```
See detailed guide: feedPaginationViaCursorAndChecksum.md
â”œâ”€â”€ Cursor-based pagination
â”œâ”€â”€ Feed versioning (optional, for UX)
â””â”€â”€ Checksums for data integrity
```

---

## Monitoring & Alerting

### Key Metrics

**Performance Metrics:**
```
1. API Response Time
   â”œâ”€â”€ p50 < 200ms
   â”œâ”€â”€ p95 < 500ms
   â””â”€â”€ p99 < 1000ms

2. Cache Hit Ratio
   â”œâ”€â”€ Redis: > 95%
   â”œâ”€â”€ Memcached: > 90%
   â””â”€â”€ Alert if < thresholds

3. Queue Processing Time
   â”œâ”€â”€ Average: < 100ms
   â”œâ”€â”€ p95: < 500ms
   â””â”€â”€ Queue depth < 10K

4. Database Query Performance
   â”œâ”€â”€ Post fetch: < 50ms
   â”œâ”€â”€ Graph query: < 100ms
   â””â”€â”€ Feed generation: < 200ms
```

**Business Metrics:**
```
1. Feed Load Success Rate: > 99.9%
2. Post Creation Success Rate: > 99.95%
3. Daily Active Users (DAU)
4. Posts per user per day
5. Real-time update delivery rate: > 95% (Redis tier)
```

### Alert Thresholds

```
CRITICAL Alerts:
â”œâ”€â”€ Feed load time > 1s â†’ Scale read capacity
â”œâ”€â”€ Queue depth > 10K â†’ Scale fanout workers
â”œâ”€â”€ Cache hit rate < 90% â†’ Investigate efficiency
â”œâ”€â”€ Celebrity post detected â†’ Monitor pull model
â””â”€â”€ Database connection pool exhausted â†’ Add connections

WARNING Alerts:
â”œâ”€â”€ Feed load time > 500ms
â”œâ”€â”€ Queue depth > 5K
â”œâ”€â”€ Cache hit rate < 95%
â””â”€â”€ Version mismatch rate > 5%
```

---

## System Evolution Path

### Phase 1: MVP (Basic System)
```
Components:
â”œâ”€â”€ Simple push model for all users
â”œâ”€â”€ Single cache layer (Redis)
â”œâ”€â”€ Basic offset pagination
â”œâ”€â”€ Synchronous fanout
â””â”€â”€ Single region deployment

Handles: ~1M users, basic functionality
```

### Phase 2: Scale Optimizations
```
Improvements:
â”œâ”€â”€ Add Memcached tier
â”œâ”€â”€ Implement message queues
â”œâ”€â”€ Async fanout workers
â”œâ”€â”€ Cursor-based pagination
â””â”€â”€ Multi-region deployment

Handles: ~50M users, improved performance
```

### Phase 3: Advanced Consistency
```
Enhancements:
â”œâ”€â”€ Feed versioning
â”œâ”€â”€ XOR checksums
â”œâ”€â”€ Hybrid push-pull model
â”œâ”€â”€ Celebrity handling
â””â”€â”€ Advanced monitoring

Handles: ~300M users, production-ready
```

### Phase 4: Real-Time Features
```
Additional Features:
â”œâ”€â”€ WebSocket integration
â”œâ”€â”€ Real-time notifications
â”œâ”€â”€ Dynamic tier migration
â”œâ”€â”€ ML-based feed ranking
â””â”€â”€ A/B testing framework

Handles: Billions of users, feature-complete
```

---

## Critical Design Decisions

### 1. Push vs Pull Trade-offs

| Aspect | Push | Pull | Hybrid |
|--------|------|------|--------|
| Read Speed | Very Fast | Slow | Fast |
| Write Cost | Very High | Low | Medium |
| Storage | High | Low | Optimized |
| Consistency | Eventual | Real-time | Eventual |
| Celebrity Handling | Poor | Excellent | Excellent |

**Decision:** Use hybrid approach
- Push for regular users (fast reads)
- Pull for celebrities (scalable writes)
- Best of both worlds

### 2. Redis + Memcached vs Single Redis

| Aspect | Single Redis | Redis + Memcached |
|--------|-------------|-------------------|
| Cost | Very High | Optimized |
| Features | Rich | Tiered |
| Complexity | Low | Medium |
| Scalability | Limited | High |

**Decision:** Multi-tier caching
- Redis for active users needing features
- Memcached for simple data storage
- Significant cost savings

### 3. Cursor vs Offset Pagination

| Aspect | Offset | Cursor |
|--------|--------|--------|
| Duplicates | Possible | No |
| Consistency | Poor | Excellent |
| Implementation | Simple | Medium |
| Performance | O(n) skip | O(log n) seek |

**Decision:** Cursor-based
- Better user experience
- Handles concurrent updates
- Industry standard

---

## Interview Deep Dive Topics

### Expected Senior Questions

**Q: "How do you handle a user with 100 million followers?"**

**A:** Use pull model instead of push:
- Don't precompute 100M feeds (write amplification)
- At read time: Fetch celebrity's recent posts + merge with user's precomputed feed
- Cache merged result for subsequent requests
- Trade-off: Slight read latency for followers, but system remains scalable

**Q: "What happens when Redis fails?"**

**A:** Graceful degradation with multiple fallback layers:
1. Redis Sentinel detects failure â†’ Automatic failover to replica
2. If entire Redis cluster down â†’ Fall back to Memcached (lose real-time features)
3. If Memcached also down â†’ Fall back to database generation
4. Hot users temporarily become warm users
5. System remains functional, just slower

**Q: "How do you prevent duplicate posts during concurrent updates?"**

**A:** Cursor-based pagination handles this naturally:
- Cursor references specific post with timestamp + ID
- Query: "posts WHERE timestamp < cursor_timestamp"
- Even if new posts inserted above, cursor position stable
- Version tracking is optional, mainly for UX (showing "new posts" banner)
- See feedPaginationViaCursorAndChecksum.md for details

**Q: "Why both Redis and Memcached instead of just scaling Redis?"**

**A:** Cost and feature optimization:
- Redis: $0.05/GB/hour, rich features (pub/sub, sessions, complex data structures)
- Memcached: $0.01/GB/hour, simple key-value
- Active users (30M) need Redis features
- Warm users (100M) only need simple storage
- Savings: ~60% reduction in cache costs
- Right tool for the right job

**Q: "How do checksums help if you already have versions?"**

**A:** Different purposes:
- **Versions:** Detect WHEN feed changed (timing)
- **Checksums:** Detect IF data is corrupted (integrity)
- Example scenario:
  - New posts arrive â†’ Version changes to v1002
  - Network error during cache update â†’ Post missing
  - Stored checksum â‰  recalculated checksum
  - System detects corruption, regenerates from database
- Versions can't catch corruption, only timing changes

**Q: "How would you implement feed ranking/ML-based feeds?"**

**A:** Evolution from chronological to ranked:
1. Keep chronological as base (this design)
2. Add ranking service:
   - Fetch chronological feed
   - Score each post (engagement, relevance, recency)
   - Re-order by score
3. Cache ranked results separately
4. A/B test: 50% chronological, 50% ranked
5. Measure engagement metrics
6. Gradually roll out if successful

---

## Summary

This news feed system design handles billions of users through:

**Key Innovations:**
- Multi-tier caching (Redis/Memcached/Database) for cost optimization
- Hybrid push-pull strategy for celebrity handling
- Cursor-based pagination for consistency (see feedPaginationViaCursorAndChecksum.md)
- Async fanout with message queues for scalability
- Tiered real-time updates (Redis only) for performance
- Graceful degradation at every layer

**Scalability Achieved:**
- 300M DAU, 150M posts/day
- 35K reads/sec, 2K writes/sec
- <500ms feed load time
- Handles celebrity users gracefully
- 54% cache cost savings vs single-tier

**Trade-offs Made:**
- Eventual consistency (acceptable for social feeds)
- Slight read latency for celebrity followers
- Complexity of multi-tier cache management
- Storage overhead for precomputed feeds

This design balances performance, cost, scalability, and user experience for a production-grade news feed system.